Through this QnA, I will try to clear common doubts which even I encountered while learning about LLMs

## Table of Contents

1. [What are LLMs and why do I see BERT being mentioned there but not used as LLM?](##what-are-llms-and-why-do-i-see-bert-being-mentioned-there-but-not-used-as-llm?)
2. [What is hallucination in LLMs?](##what-is-hallucination-in-llms)
3. [Why do we call LLMs as statistical language modeling?](##why-do-we-call-llms-as-statistical-language-modeling)
4. [What are embeddings?](##what-are-embeddings)
5. [What does Ingest.py does?](##what-does-ingestpy-does)
6. [What are Vector Stores?](#what-are-vector-stores)
7. [What are Vector Databases?](#what-are-vector-databases)
8. [Vector vs Traditional Database](#vector-vs-traditional-database)
9. [What are some examples of Vector Stores?](#what-are-some-examples-of-vector-stores)
10. [What is Chunk size and Chunk overlap?](#what-is-chunk-size-and-chunk-overlap)
11. [Few famous Embedding models?](#few-famous-embedding-models)
12. [What is AutoModelForCausalLM?](#what-is-automodelforcausallm)
13. [What is Llamacpp?](#what-is-llamacpp)
14. [What is CTransformers?](#what-is-ctransformers)
15. [What is GGUF and GGML?](#what-is-gguf-and-ggml)
16. [What is KV cache?](#what-is-kv-cache)
17. [What does KV KM means while downloading models?](#what-does-kv-km-means-while-downloading-models)
18. [How does caching the LLM model helps?](#how-does-caching-the-llm-model-helps)
19. [What are different types of Memory available?](#what-are-different-types-of-memory-available)

## What are LLMs and why do I see BERT being mentioned there but not used as LLM?
    
    LLMs (for example GPTs) are decoder-only models which have been trained with tons of unstructured data.
    What I mean by Decoder-only model is that, given an input they output a range of probability values for the next work.
    Encoder models take your input and convert in into Embeddings.

    Vanilla or original BERT is encoder-only model and it is preferred by many devs to generate embeddings due to its embedding dimension of 768. Embedding dimension means that each word will have 768 characteristics being described by the model.

    Since BERT is encoder only model, its cannot be used for a decoding task such as generating sentences.
    
    For BERT, you can watch this video by Umar Jamil [BERT video](https://www.youtube.com/watch?v=90mGPxR2GgY)

    Most important, LLMs were built to predict the next word and hence it has quite some limitations such as hallucinations and chain of thought.

## What is hallucination in LLMs?

    When the LLM model start giving words/sentences that is completely out of the context or factually wrong.
    This happens because of either the dataset the model was trained on or the model's interpreting capabilities.

## Why do we call LLMs as statistical language modeling?

    The output generated by the decoder-only models are based on probability. Given an input sentence and model's vocabulary, the model will calculate which is the next word in the sentence by assigning probabilities to all the words in its vocabulary. Then it will select the word with the highest probability value.

What are embeddings?




What does Ingest.py does?

    While working with RAG, you have to generate some embeddings or data about the task you are going to perform.
    You may be working on some task on which the model is not trained or the training data is older.
    So what you do is, take your data (maybe PDF/CSV/Text files) and generate embeddings out of it.
    These embeddings are then stored in a vector store or database to help you fetch it whenever required.
    So, ingest.py helps you generate and store embeddings in given database/data store.

    If reads your data, split it into smaller parts, generate embeddings and store them.

What are Vector Stores?

What are Vector Databases?

Vector vs Traditional Database

What are some examples of Vector Stores?

    FAISS
    Chroma

What is Chunk size and Chunk overlap?

    Chunk size - While generating embeddings, how many tokens will constitute to 1 block/document

    Chunk overlap - When the text is too big (more than chunk size, technically), we divide it into different documents/blocks.
                    Chunk overlap denotes how many tokens to overlap with the previous block/document to maintain continuity.

Few famous Embedding models?

What is AutoModelForCausalLM?

What is Llamacpp?

    Llamacpp is a library meant to help devs load various models and run them on various hardwares.
    It focuses on optimization - speed and performance

    Parameters while loading a model

    model_path : path to the model
    temperature : decides how deterministic or creative the output as to be. 0.1 - deterministic/conservative output and 0.9 - more creative and different output
    max_tokens : maximum num of tokens to generate
    top_p : The model outputs a set of next tokens and their probabilities. Top_p selects a set of token such that their      probability scores sum up to 1. Then choose 1 token from it and return a output
    n_gpu_layers : num of layers to load in GPU
    n_batch : number of tokens to process in parallel. Should be less than max_tokens
    verbose : print the info while loading the model

What is CTransformers?

What is GGUF and GGML?

What is KV cache?

What does KV KM means while downloading models?

How does caching the LLM model helps?

What are different types of Memory available?
    
    ConversationBufferMemory
    ConversationBufferWindowMemory
    ConversationSummaryMemory
    ConversationSummaryBufferMemory

    // https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/