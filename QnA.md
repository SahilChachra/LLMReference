Through this QnA, I will try to clear common doubts which even I encountered while learning about LLMs

## Table of Contents

- [What are LLMs and why do I see BERT being mentioned there but not used as LLM?](#what-are-llms-and-why-do-i-see-bert-being-mentioned-there-but-not-used-as-llm)
- [What is hallucination in LLMs?](#what-is-hallucination-in-llms)
- [Why do we call LLMs as statistical language modeling?](#why-do-we-call-llms-as-statistical-language-modeling)
- [What are tokens? What is Tokenization?](#what-are-tokens-what-is-tokenization)
- [What are embeddings?](#what-are-embeddings)
- [What is RAG?](#what-is-rag)
- [What are Vector Stores/Database?](#what-are-vector-storesdatabase)
- [Vector vs Traditional Database](#vector-vs-traditional-database)
- [What are some examples of Vector Stores and Databases?](#what-are-some-examples-of-vector-stores-and-databases)
- [What is Chunk size and Chunk overlap?](#what-is-chunk-size-and-chunk-overlap)
- [Few famous Embedding models used for RAGs?](#few-famous-embedding-models-used-for-rags)
- [What does Ingest.py do?](#what-does-ingestpy-do)
- [What is AutoModelForCausalLM?](#what-is-automodelforcausallm)
- [What is Llamacpp?](#what-is-llamacpp)
- [What is CTransformers?](#what-is-ctransformers)
- [What is GGUF and GGML?](#what-is-gguf-and-ggml)
- [What is KV cache?](#what-is-kv-cache)
- [What does KV KM mean while downloading models?](#what-does-kv-km-mean-while-downloading-models)
- [How does caching the LLM model help?](#how-does-caching-the-llm-model-help)
- [What are different types of Memory available?](#what-are-different-types-of-memory-available)


## What are LLMs and why do I see BERT being mentioned there but not used as LLM?
    
    LLMs (for example GPTs) are decoder-only models which have been trained with tons of unstructured data.
    What I mean by Decoder-only model is that, given an input they output a range of probability values for the next work.
    Encoder models take your input and convert in into Embeddings.

    Vanilla or original BERT (Bidirectional Encoder Representations from Transformers), developed by Google, is encoder-only model and it is preferred by many devs to generate embeddings due to its embedding dimension of 768. Embedding dimension means that each word will have 768 characteristics being described by the model.

    Since BERT is encoder only model, its cannot be used for a decoding task such as generating sentences.
    
    For BERT, you can watch this video by Umar Jamil [BERT video](https://www.youtube.com/watch?v=90mGPxR2GgY)

    Most important, LLMs were built to predict the next word and hence it has quite some limitations such as hallucinations and chain of thought.

## What is hallucination in LLMs?

    When the LLM model start giving words/sentences that is completely out of the context or factually wrong.
    This happens because of either the dataset the model was trained on or the model's interpreting capabilities.

## Why do we call LLMs as statistical language modeling?

    The output generated by the decoder-only models are based on probability. Given an input sentence and model's vocabulary, the model will calculate which is the next word in the sentence by assigning probabilities to all the words in its vocabulary. Then it will select the word with the highest probability value.


## What are tokens? What is Tokenization?

    When you write an essay, you write it word by word. When you read something, you do it word by word. Meaning, you break down the sentence into words and then try to capture its meaning. Same thing in NLP also! The data/sentences are broken down into words and even words are broken down into smaller units known as tokens

    Tokenization is the process of breaking down words into tokens.
    Tokenization can be done in many ways, such as splitting the words in the sentence by spaces, some special characters or just the words itself, or combination of multiple separators.

    Sub Words Tokenization

        This methods breaks a single word into smaller units or sub-tokens. It is done because many words have such prefixes and suffixes which can be added to other words as well and their meaning changes! This helps us handle out of vocabulary words. There are many algos available but their usage depends on the task at hand - WordPiece, Byte Pair Encoding (BPE), Unigram Sub-word Tokenization, SentencePiece

        BPE is used in GPTs, LLaMA, BART (Bidirectional and Auto-Regressive Transformers)
        WordPiece is used by BERT
        SentencePiece is used by T5, A lite BERT


## What are embeddings?

    Embeddings are representation of data. In the case of LLMs, the input is converted into embeddings of dimension 768 (for ex. if we are using BERT). It means that each word or token will be represented by 768 characteristics or properties which will define what it means.

    We have two type of Embeddings in Language - Contenxtual and Non Contextual
    Contextual means - the model can different the meaning of bank in the context of finance and in the context of river.
    Non-contextual means - the embeddings are static. It cannot differentiate a word which mean two different things in two different context.

    Contextual models are - BERT and its variants
    Non-Contextual mdoels ar e- Word2Vec, GloVe

    These embeddings are nothing by floating point values.
    In other words, embeddings are stored as vectors such that when projected in some space, you can see similar words being group together.

    Since these are vectors, it is possible to apply arthmetic operations on them such as Cosine-similarity which can be used to calculate it the Similarity-Search in many tasks.

## What is RAG?

    RAG or Retreievel Augment Generation. It is a method by which the decoder-only model uses external knowledge to answer questions. External knowledge is so such that the model is able to answer questions on which it was never trained on. So instead of training the model, we provide external data source which is used by model. By this menthod, same model can be used for multiple tasks.

    Steps :-
        1. Gather all your data and make sure they are of valid format
        2. Pass the data to a tokenizer
        3. Generated embeddings from the tokens using an embedding model and store in vector store or database
        4. Take the input question and generate its embedding using the same model which was used to create embeddings of the data/knowledge base
        5. Run a query in the vector database to get relevant data from the store/database. Here Approximate Nearest Neighbour algorithms are used
        6. The returned vector data are mapped against the words
        7. Pass the question, prompt and retrieved embeddings to the LLM model. The model will use the prompt the return the relevant answer from the retrieved data

## What are Vector Stores/Database?

    Vector stores and databases store vector type data and uses algorithms to index and query data. The use ANN or Approximate Nearest Neighbour search through hasing or graph based search. You can generate embeddings of image/audio/text and store them as vectors in these stores/databases.

    Databases are used if you want to scale up your application. If you are working on a POC or small scale project, you can use vector stores/

## Vector vs Traditional Database

    Traditional databases are meant for storing general-purpose data which can be queried using SQL. 
    
    Vector databases are meant to store high dimensional data. These are optimized for data storage and retrieval and also enables similarity search algorithms. Designed to scale with large vector databases.

## What are some examples of Vector Stores and Databases?

    Stores are :-
        FAISS
        Chroma
        Pinecone
        Milvus
    
    Databases :-
        Qdrant
        Supabase

    Additional info : [VectorDB comparisons](https://superlinked.com/vector-db-comparison/)

## What is Chunk size and Chunk overlap?

    Chunk size - While generating embeddings, how many tokens will constitute to 1 block/document

    Chunk overlap - When the text is too big (more than chunk size, technically), we divide it into different documents/blocks.
                    Chunk overlap denotes how many tokens to overlap with the previous block/document to maintain continuity.

Few famous Embedding models used for RAGs?

    TO-DO

## What does Ingest.py does?

    While working with RAG, you have to generate some embeddings or data about the task you are going to perform.
    You may be working on some task on which the model is not trained or the training data is older.
    So what you do is, take your data (maybe PDF/CSV/Text files) and generate embeddings out of it.
    These embeddings are then stored in a vector store or database to help you fetch it whenever required.
    So, ingest.py helps you generate and store embeddings in given database/data store.

    If reads your data, split it into smaller parts, generate embeddings and store them.

## What is AutoModelForCausalLM?

## What is Llamacpp?

    Llamacpp is a library meant to help devs load various models and run them on various hardwares.
    It focuses on optimization - speed and performance

    Parameters while loading a model

    model_path : path to the model
    temperature : decides how deterministic or creative the output as to be. 0.1 - deterministic/conservative output and 0.9 - more creative and different output
    max_tokens : maximum num of tokens to generate
    top_p : The model outputs a set of next tokens and their probabilities. Top_p selects a set of token such that their      probability scores sum up to 1. Then choose 1 token from it and return a output
    n_gpu_layers : num of layers to load in GPU
    n_batch : number of tokens to process in parallel. Should be less than max_tokens
    verbose : print the info while loading the model

## What is CTransformers?

## What is GGUF and GGML?

## What is KV cache?

## What does KV KM means while downloading models?

## How does caching the LLM model helps?

## What are different types of Memory available?
    
    ConversationBufferMemory
    ConversationBufferWindowMemory
    ConversationSummaryMemory
    ConversationSummaryBufferMemory

    // https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/